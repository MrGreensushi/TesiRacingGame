{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fea2a4-8e09-4803-be40-899fe4695d84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2e55b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import Generator as Generator \n",
    "import model_definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f6852",
   "metadata": {},
   "source": [
    "### Open input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7838b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PcPath:\n",
    "    def __init__(self,physic,commPhy):\n",
    "        self.physic=physic\n",
    "        self.commPhy=commPhy\n",
    "        \n",
    "\n",
    "\n",
    "DANIELE=PcPath(r\"C:\\Users\\dansp\\OneDrive\\Desktop\\Tesi\\Logs\\OneCar_db.txt\",\n",
    "               r\"C:\\Users\\dansp\\OneDrive\\Desktop\\Tesi\\Logs\\PhysycAndCommand_db.txt\")\n",
    "PCS=[DANIELE]\n",
    "PC_INDEX=0\n",
    "\n",
    "COLUMNS_PHY=[\"Player\", \"X\", \"Z\", \"VEL_X\",\"VEL_Z\",\"ROT\",\"ANG_VEL_Y\",\"ACC_X\",\"ACC_Z\",\"TILE\",\"TILE_IND\",\"X_RELATIVE\",\"Z_RELATIVE\",\"TIME\"]\n",
    "\n",
    "COLUMNS_COMM_PHY=[\"Player\", \"X\", \"Z\", \"VEL_X\",\"VEL_Z\",\"ROT\",\"ANG_VEL_Y\",\"ACC_X\",\"ACC_Z\",\"TILE\",\"TILE_IND\",\"X_RELATIVE\",\"Z_RELATIVE\",\"TIME\",\"MOVE_X\",\"MOVE_Z\",\"BREAKING\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41825ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH=20\n",
    "ONE_CAR=True\n",
    "PATH=r\"D:\\Users\\Students\\Daniele\\records\\OneCar_db.txt\"\n",
    "#PATH=r\"C:\\Users\\dansp\\OneDrive\\Desktop\\Tesi\\SCRITTURA\\OneCar_tack1_db.txt\"\n",
    "BATCH_SIZE=256*2\n",
    "FEATURES=7 \n",
    "FEATURES_PREDICTED=6\n",
    "DISCARD=4"
    "COMM_PHY=True\n",
    "#PATH=r\"C:\\Users\\dansp\\OneDrive\\Desktop\\Tesi\\SCRITTURA\\OneCar_tack1_db.txt\"\n",
    "BATCH_SIZE=256*2\n",
    "FEATURES=7 \n",
    "FEATURES_PREDICTED=3\n",
    "DISCARD=9\n",
    "\n",
    "if(COMM_PHY):\n",
    "    PATH=PCS[PC_INDEX].commPhy\n",
    "    FEATURES +=3 #adding the command to the features \n",
    "    COLUMNS=COLUMNS_COMM_PHY\n",
    "else:\n",
    "    PATH=PATH=PCS[PC_INDEX].physic\n",
    "    COLUMNS=COLUMNS_PHY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252787ad-4829-467c-8bfa-c15a35435c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Generator.SEQUENCE_LENGTH=SEQUENCE_LENGTH\n",
    "Generator.FEATURES=FEATURES\n",
    "Generator.DISCARD=DISCARD\n",
    "Generator.COLUMNS=COLUMNS\n",
    "Generator.COMM_PHY=COMM_PHY\n",
    "train,val,test=Generator.single_care_dataframe(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606f34c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SubtractDF(df):\n",
    "    return df.groupby(\"RACE\",group_keys=False).apply(Generator.subtraction_columns)\n",
    "\n",
    "def SubtractAndBuildDF(df):\n",
    "    dfs=[]\n",
    "    for i in range(len(df)):\n",
    "        dfs.append(SubtractDF(df[i]))\n",
    "        #dfs.append(df[i])\n",
    "    united=dfs[0]\n",
    "    for i in range(1,len(df)):\n",
    "        united=pd.concat([united,dfs[i] ],ignore_index=True)\n",
    "    return united\n",
    "\n",
    "df_train= SubtractAndBuildDF(train)\n",
    "df_val= SubtractAndBuildDF(val)\n",
    "df_test= SubtractAndBuildDF(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a142c-90ad-41ed-bdd0-89476ac9e60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e38c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train=Generator.DataGenerator(BATCH_SIZE,df_train,10000)\n",
    "gen_val=Generator.DataGenerator(BATCH_SIZE,df_val,2600)\n",
    "gen_test=Generator.DataGenerator(BATCH_SIZE,df_test,2600)\n"
   ]
  },
  {

   "attachments": {},
   "cell_type": "markdown",
   "id": "1ce93477",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f400a4",
   "metadata": {},
   "source": [
    "#### Train and print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d50ad3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if(ONE_CAR):\n",
    "    model_definitions.cars=1\n",
    "else:\n",
    "    model_definitions.cars=4\n",
    "model_definitions.features=FEATURES\n",
    "model_definitions.sequence_length=SEQUENCE_LENGTH\n",
    "model_definitions.feat_pred=FEATURES_PREDICTED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d92d02-082b-47d0-9a36-b42cf8349a43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Rules Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm,history=LSTM_Train(2,SEQUENCE_LENGTH,4,rules_train,rules_val,learning_rate=0.01,epoch=100, dropout=0.2, cells=6 )\n",
    "\n",
    "ev=lstm.evaluate(rules_test,verbose=0)\n",
    "print(f\"Loss: {ev[0]} MAE: {ev[1]} Accuracy: {ev[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm2,history=LSTM_Train(2,SEQUENCE_LENGTH,4,rules_train,rules_val,learning_rate=0.002,epoch=100, dropout=0.02, cells=1)\n",
    "\n",
    "ev=lstm2.evaluate(rules_test,verbose=0)\n",
    "print(f\"Loss: {ev[0]} MAE: {ev[1]} Accuracy: {ev[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc81020",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp,history=MLP_Train(2,SEQUENCE_LENGTH,4,rules_train,rules_val,learning_rate=0.001,epoch=100)\n",
    "\n",
    "ev=mlp.evaluate(rules_test,verbose=0)\n",
    "print(f\"Loss: {ev[0]} MAE: {ev[1]} Accuracy: {ev[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dedac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese,history=LSTM_Siamese_Train(2,SEQUENCE_LENGTH,4,rules_train,rules_val,learning_rate=0.001,epoch=100, dropout=0.02, cells=2 )\n",
    "\n",
    "ev=siamese.evaluate(rules_test,verbose=0)\n",
    "print(f\"Loss: {ev[0]} MAE: {ev[1]} Accuracy: {ev[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fbcaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Physichs Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543ee461",
   "metadata": {},
   "source": [
    "Since physichs data can be very small, Each data is multiply by 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc5a7d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NAME=f\"MLP_DISCARD_{DISCARD}_DELTA_LONG_TRAIN\"\n",
    "mlp_ph,history_mlp_ph=model_definitions.MLP_Train(gen_train,gen_val,learning_rate=0.0001,epoch=1000,verbose=1,name=NAME,path=\"logs\")\n",
    "\n",
    "ev=mlp_ph.evaluate(gen_test,verbose=0)\n",
    "print(f\"Loss: {ev[0]} MAE: {ev[1]} Accuracy: {ev[2]}\")\n",
    "tf.saved_model.save(mlp_ph,f'saved_model/{NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220477fd-dcf0-43ba-8a66-75e4d7745bca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "units=32\n",
    "kernel_size=1\n",
    "filters=32\n",
    "normalization=False\n",
    "cnn_lstm,history_cnn_lstm=model_definitions.CNN_LSTM_Train(gen_train,gen_val,learning_rate=0.0001,patience=10,name=f\"CNN{filters}_{kernel_size}LSTM{units}_Norm{normalization}\",\n",
    "                                                           path=\"logs_350SequenceLength\",epoch=10, dropout=0.2,units=units,kernel_size=kernel_size,\n",
    "                                                           filters=filters, normalization=normalization,verbose=1\n",
    "                                                          )\n",
    "\n",
    "ev=cnn_lstm.evaluate(gen_test,verbose=0)\n",
    "print(f\"Loss: {ev[0]} MAE: {ev[1]} Accuracy: {ev[2]}\")\n",
    "\n",
    "tf.saved_model.save(cnn_lstm,f'saved_model/CNN_LSTM{units}_{kernel_size}_{filters}_PosAssoluta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695dc32",
   "metadata": {
    "tags": []
   },

   "outputs": [],
   "source": [
    "lr =1e-4\n",
    "units=64\n",
    "cells=3\n",
    "normalization=False\n",
    "mlp_units=64\n",
    "mlp_cells=12\n",
    "name=f\"LSTM{cells}_{units}_{lr}_{DISCARD}_MLP_{mlp_cells}_{mlp_units}\"\n",
    "lstm_ph,history_lstm_ph=model_definitions.LSTM_Train(\n",
    "    gen_train,\n",
    "    gen_val,\n",
    "    learning_rate=lr,\n",
    "    patience=6,\n",
    "    name=name,\n",
    "    path=f\"new_split_Tile_info_DISCARD{DISCARD}\",\n",
    "    epoch=100, \n",
    "    dropout=0.2, \n",
    "    cells=cells,\n",
    "    units=units,\n",
    "    mlp_units=mlp_units,\n",
    "    mlp_cells=mlp_cells,\n",
    "    normalization=normalization,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ev=lstm_ph.evaluate(gen_test,verbose=0)\n",
    "print(f\"Loss: {ev[0]} MAE: {ev[1]} Accuracy: {ev[2]}\")\n",
    "#tf.saved_model.save(lstm_ph,f'saved_model/{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ab8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=0\n",
    "for batch in gen_val:\n",
    "    print(test)\n",
    "    test+=1\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f170073",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lstm_ph.predict(gen_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df7839-f920-48fc-a703-e666e4b23d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =1e-3\n",
    "units=512\n",
    "cells=8\n",
    "normalization=False\n",
    "name=f\"LSTM{cells}_{units}_{lr}_DISCARD_{DISCARD}_DELTA\"\n",
    "lstm_ph,history_lstm_ph=model_definitions.LSTM_Train(gen_train,gen_val,learning_rate=lr,patience=50,name=name,path=\"logs\",\n",
    "                                                     epoch=100, dropout=0.2, cells=cells,units=units, normalization=normalization,verbose=1)\n",
    "\n",
    "ev=lstm_ph.evaluate(gen_test,verbose=0)\n",
    "print(f\"Loss: {ev[0]} MAE: {ev[1]} Accuracy: {ev[2]}\")\n",
    "tf.saved_model.save(lstm_ph,f'saved_model/{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df7839-f920-48fc-a703-e666e4b23d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(lstm_ph,f'saved_model/{name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_definitions.MLP_Model(32)\n",
    "model.load_weights(\"saved_model\\MLP0001_PosAssoluta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero=gen_test[0]\n",
    "toP=zero[0][0]\n",
    "pred=zero[1][0]\n",
    "print(toP)\n",
    "print(pred)\n",
    "model.predict(toP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ac715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "input_signature=(tf.TensorSpec([None,SEQUENCE_LENGTH,FEATURES],tf.float32,name='x'))\n",
    "onnx_model,_=tf2onnx.converter.from_keras(model,input_signature,opset=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7adab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa35bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "races=united[\"RACE\"].max()\n",
    "print(races*0.5)\n",
    "training=united.loc[united['RACE'] <= races*0.5]\n",
    "training.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=df_train[\"ROT\"].idxmax()\n",
    "df_train.iloc[t-2:t+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93ae5470-5bb8-49c8-89ab-0078b74c8ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "SEQUENCE_LENGTH=20\n",
    "ONLY_ONE_CAR=True\n",
    "CARS=4\n",
    "FEATURES=5\n",
    "DISCARD=9\n",
    "COLUMNS=[\"Player\", \"X\", \"Z\", \"VEL_X\",\"VEL_Z\",\"ROT\",\"ANG_VEL_Y\",\"ACC_X\",\"ACC_Z\",\"TILE\",\"TILE_IND\",\"X_RELATIVE\",\"Z_RELATIVE\",\"TIME\"]\n",
    "C_NoPLayer=[\"X\", \"Z\", \"VEL_X\",\"VEL_Z\",\"ROT\",\"ANG_VEL_Y\",\"ACC_X\",\"ACC_Z\",\"TILE\",\"TILE_IND\",\"X_RELATIVE\",\"Z_RELATIVE\",\"TIME\",\"RACE\",\"GROUP\"]\n",
    "\n",
    "def normalize_df(df,minimum,maximum):\n",
    "    normalized=(df-minimum)/(maximum-minimum)\n",
    "    normalized[\"RACE\"]=df[\"RACE\"]\n",
    "    return normalized\n",
    "\n",
    "      \n",
    "    \n",
    "def single_care_dataframe(path):\n",
    "    df_gara=pd.read_csv(path, sep=\";\", header=None , decimal=',',names=COLUMNS)\n",
    "    #create new column called race\n",
    "    df_gara[\"RACE\"]=0\n",
    "    # fill race column based on the cumulative sum of rows starting with '_'\n",
    "    #idx_gara stores each row wich starts with '_'\n",
    "    idx_gara=(df_gara[df_gara[\"Player\"].str.startswith(\"_\")].index)\n",
    "    df_gara.loc[idx_gara,\"Player\"]=df_gara.loc[idx_gara,\"Player\"].str.replace(\"_\",\"\")#replace name without the _\n",
    "\n",
    "    df_gara.loc[idx_gara,\"RACE\"]=1\n",
    "    df_gara.RACE=df_gara.RACE.cumsum()\n",
    "    #Create new column Length wich specifiens the total length of a race\n",
    "    df_gara[\"LENGTH\"]=df_gara.groupby(\"RACE\")[\"Player\"].transform(\"count\")\n",
    "    #if race is lewer then a minimum then it is discarded\n",
    "    df_races=df_gara.query(f\"LENGTH > {SEQUENCE_LENGTH*DISCARD+1}\").reset_index(drop=True)\n",
    "    df_races.drop([\"Player\",\"LENGTH\"],axis=1,inplace=True)\n",
    "    print(df_races[\"RACE\"].nunique())\n",
    "    print(df_races[\"RACE\"].max())\n",
    "\n",
    "    train,val,test = split_train_validation_test(\n",
    "        df_races,\n",
    "        \"RACE\"\n",
    "    ) \n",
    "    df_train= divide_into_groups(train)\n",
    "    df_val= divide_into_groups(val)\n",
    "    df_test= divide_into_groups(test)\n",
    "\n",
    "    return df_train, df_val,df_test\n",
    "\n",
    "def divide_into_groups(df_x):\n",
    "    df=df_x.copy()\n",
    "    dfs=[]\n",
    "    for i in range(DISCARD+1):\n",
    "        df[\"GROUP\"]=i\n",
    "        temp=df.iloc[i::DISCARD+1]\n",
    "        temp.reset_index(drop=True,inplace=True)\n",
    "        dfs.append(temp)\n",
    "    return dfs\n",
    "\n",
    "def subtraction_columns(df):\n",
    "    df_copy=df.shift(1,fill_value=0)\n",
    "    cols = df.columns.difference(['RACE','GROUP',\"TILE\",\"TILE_IND\",\"X_RELATIVE\",\"Z_RELATIVE\"])\n",
    "    df[cols] = df[cols].sub(df_copy[cols])\n",
    "    df[\"ROT\"]=(df[\"ROT\"]+180)%360-180\n",
    "    df.iloc[0,:-7]=0\n",
    "    df.iloc[0,10:12]=0\n",
    "    df.iloc[0,-3]=df.iloc[1,-3]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_split(x,first,second):\n",
    "    first =int(x.shape[0]*first)\n",
    "    second = int(x.shape[0]*second) \n",
    "    return x[first:second]\n",
    "    \n",
    "def split_train_validation_test(df,group_col,train_split=0.5,val_split=0.25,test_split=0.25):\n",
    "    val_split +=train_split\n",
    "    \n",
    "    if val_split >1:\n",
    "        raise ValueError(\n",
    "            f\"Train + Validation split cannot be higher tan 1 given {val_split}\"\n",
    "        )\n",
    "\n",
    "    races=df[\"RACE\"].max()+1\n",
    "    df_train=df.loc[df['RACE'] < races*train_split]\n",
    "    df_val=df.loc[(df['RACE'] >= races*train_split) & (df['RACE'] < races*val_split)]\n",
    "    df_test=df.loc[df['RACE'] >= races*val_split]\n",
    "    \n",
    "    #group by race (and player name for single car) and create a new array containing foreach race a dataset\n",
    "    #df_train= df.groupby(group_col,group_keys=False).apply(get_split, first = 0, second= train_split)\n",
    "    #df_val= df.groupby(group_col,group_keys=False).apply(get_split, first = train_split, second= val_split)\n",
    "    #df_test= df.groupby(group_col,group_keys=False).apply(get_split, first = val_split, second= 1)\n",
    "    #\n",
    "    #\n",
    "    ##since each race was plittend into train,val and test the result of previous operation is an array containing the data \n",
    "    ##foreach race, therefore to have the end dataframe we must concatenate each element\n",
    "    #df_train=recreate_dataframe(df_train)\n",
    "    #df_val=recreate_dataframe(df_val)\n",
    "    #df_test=recreate_dataframe(df_test)\n",
    "    \n",
    "    return df_train, df_val,df_test\n",
    "\n",
    "def recreate_dataframe(series):\n",
    "    columns=C_NoPLayer \n",
    "    series.columns=columns\n",
    "    df=series.reset_index(drop=True)\n",
    "    #v#alues=series.values\n",
    "    #df=pd.DataFrame(values[0],columns=columns)\n",
    "    #\n",
    "#\n",
    "    #for serie in values[1:]:\n",
    "    #    df= pd.concat([df, pd.DataFrame(serie,columns=columns)],ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def batch_generator(df):\n",
    "    \n",
    "    #crea un nuovo dataframe con sequence_length elementi per un numero di volte pari al batch\n",
    "    dropped_df=df.drop([\"TIME\",\"RACE\",\"GROUP\",\"X\",\"Z\",\"ACC_X\",\"ACC_Z\",\"ANG_VEL_Y\"],axis=1).reset_index(drop=True)\n",
    "    #dropped_df[\"ROT\"]=dropped_df[\"ROT\"]/360.0;\n",
    "    target_df=dropped_df.drop([\"TILE\",\"TILE_IND\",\"X_RELATIVE\",\"Z_RELATIVE\"],axis=1).reset_index(drop=True)\n",
    "    #dropped_df=dropped_df.drop([ \"VEL_X\",\"VEL_Z\",\"ROT\"],axis=1).reset_index(drop=True)\n",
    "    for i in range(len(dropped_df)-SEQUENCE_LENGTH):\n",
    "        inputs=np.array(dropped_df.loc[i:SEQUENCE_LENGTH-1+i,:].values)\n",
    "        targets=target_df.iloc[SEQUENCE_LENGTH+i,:].values\n",
    "        yield inputs,targets  \n",
    "\n",
    "def Generator(df):\n",
    "    grouped=df.groupby([\"RACE\",\"GROUP\"],group_keys=False).apply(batch_generator)\n",
    "    for group in grouped:\n",
    "        for single in group:\n",
    "            yield single\n",
    "            \n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self,batch_size,df,max_batch):\n",
    "        self.batch_size=batch_size\n",
    "        self.df=df\n",
    "        length=df.groupby([\"RACE\",\"GROUP\"]).apply(countSize)\n",
    "        \n",
    "        self.df_length=len(df.index)-(df[\"RACE\"].nunique()*SEQUENCE_LENGTH*CARS)\n",
    "        self.max_batch=max_batch            \n",
    "        print(f'Length: {len(df.index)} races: {df[\"RACE\"].nunique()} n batches: {self.df_length} / {batch_size}')\n",
    "        self.on_epoch_end()\n",
    "        #self.generator=generator_function(sequence_length,path,totFiles)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        X=[]\n",
    "        Y=[]\n",
    "        for i in range(self.batch_size):\n",
    "            #while True:\n",
    "            #    x,y=next(self.generator)\n",
    "            #    x_shape=np.shape(x)\n",
    "            #    if x_shape[0]==x_shape[1]:\n",
    "            #        break\n",
    "            x,y=next(self.generator)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            \n",
    "        #print(np.shape(X))    \n",
    "        tensor_x=tf.constant(X)\n",
    "        tensor_y=tf.constant(Y)\n",
    "        return tensor_x,tensor_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        value=int(self.df_length/self.batch_size-1)\n",
    "        if value>self.max_batch:\n",
    "            value=self.max_batch\n",
    "        return value\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.generator=Generator(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a254609-0495-44c3-85e9-52ce48782d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2703516"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(len(df_train.index) - df_train[\"RACE\"].nunique()*SEQUENCE_LENGTH)/BATCH_SIZE\n",
    "def countSize(df):\n",
    "    return len(df.reset_index(drop=True))-SEQUENCE_LENGTH\n",
    "\n",
    "somma=df_train.groupby([\"RACE\",\"GROUP\"]).apply(countSize)\n",
    "somma.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5328818-c0ec-4e0b-8c9d-1b27f83d92b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Z</th>\n",
       "      <th>VEL_X</th>\n",
       "      <th>VEL_Z</th>\n",
       "      <th>ROT</th>\n",
       "      <th>ANG_VEL_Y</th>\n",
       "      <th>ACC_X</th>\n",
       "      <th>ACC_Z</th>\n",
       "      <th>TILE</th>\n",
       "      <th>TILE_IND</th>\n",
       "      <th>X_RELATIVE</th>\n",
       "      <th>Z_RELATIVE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>RACE</th>\n",
       "      <th>GROUP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2720311</th>\n",
       "      <td>-3.02600</td>\n",
       "      <td>4.787650</td>\n",
       "      <td>0.228000</td>\n",
       "      <td>-0.06486</td>\n",
       "      <td>-0.8624</td>\n",
       "      <td>0.594987</td>\n",
       "      <td>2.488040</td>\n",
       "      <td>-1.159763</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.226296</td>\n",
       "      <td>0.376615</td>\n",
       "      <td>0.2</td>\n",
       "      <td>83</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720312</th>\n",
       "      <td>-2.67359</td>\n",
       "      <td>4.945070</td>\n",
       "      <td>2.600840</td>\n",
       "      <td>1.12468</td>\n",
       "      <td>7.7518</td>\n",
       "      <td>0.070049</td>\n",
       "      <td>5.761860</td>\n",
       "      <td>3.362847</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.190648</td>\n",
       "      <td>0.442550</td>\n",
       "      <td>0.2</td>\n",
       "      <td>83</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720313</th>\n",
       "      <td>-1.90893</td>\n",
       "      <td>5.190311</td>\n",
       "      <td>4.206075</td>\n",
       "      <td>1.07436</td>\n",
       "      <td>9.8041</td>\n",
       "      <td>-0.011295</td>\n",
       "      <td>-8.786300</td>\n",
       "      <td>-4.183674</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.165195</td>\n",
       "      <td>-0.476492</td>\n",
       "      <td>0.2</td>\n",
       "      <td>83</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720314</th>\n",
       "      <td>-1.24964</td>\n",
       "      <td>5.344787</td>\n",
       "      <td>2.852189</td>\n",
       "      <td>0.50855</td>\n",
       "      <td>6.1715</td>\n",
       "      <td>0.238537</td>\n",
       "      <td>11.289770</td>\n",
       "      <td>-2.462674</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.148534</td>\n",
       "      <td>-0.333965</td>\n",
       "      <td>0.2</td>\n",
       "      <td>83</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720315</th>\n",
       "      <td>-0.57885</td>\n",
       "      <td>5.431946</td>\n",
       "      <td>3.084546</td>\n",
       "      <td>0.64175</td>\n",
       "      <td>5.5514</td>\n",
       "      <td>-0.873222</td>\n",
       "      <td>-14.921353</td>\n",
       "      <td>2.689744</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140815</td>\n",
       "      <td>-0.189113</td>\n",
       "      <td>0.2</td>\n",
       "      <td>83</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X         Z     VEL_X    VEL_Z     ROT  ANG_VEL_Y      ACC_X  \\\n",
       "2720311 -3.02600  4.787650  0.228000 -0.06486 -0.8624   0.594987   2.488040   \n",
       "2720312 -2.67359  4.945070  2.600840  1.12468  7.7518   0.070049   5.761860   \n",
       "2720313 -1.90893  5.190311  4.206075  1.07436  9.8041  -0.011295  -8.786300   \n",
       "2720314 -1.24964  5.344787  2.852189  0.50855  6.1715   0.238537  11.289770   \n",
       "2720315 -0.57885  5.431946  3.084546  0.64175  5.5514  -0.873222 -14.921353   \n",
       "\n",
       "            ACC_Z  TILE  TILE_IND  X_RELATIVE  Z_RELATIVE  TIME  RACE  GROUP  \n",
       "2720311 -1.159763     4        32    0.226296    0.376615   0.2    83      9  \n",
       "2720312  3.362847     4        32    0.190648    0.442550   0.2    83      9  \n",
       "2720313 -4.183674    16         0    0.165195   -0.476492   0.2    83      9  \n",
       "2720314 -2.462674    16         0    0.148534   -0.333965   0.2    83      9  \n",
       "2720315  2.689744    16         0    0.140815   -0.189113   0.2    83      9  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
